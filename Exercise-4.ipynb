{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Exercise 4 includes a **written assignment** (10 points), a **programming assignment with 5 problems** (9 points) and a **feedback/workload assessment assignment** (1 point). For each problem you need to modify the notebook by adding your own programming solutions or written text. Remember to save and commit your changes locally, and push your changes to GitHub after each major change! Regular commits will help you to keep track of your changes (and revert them if needed). Pushing your work to GitHub will ensure that you don't lose any work in case your computer crashes (can happen!).\n",
    "\n",
    "### Due date\n",
    "\n",
    "This exercise should be returned to your personal Github repository **within two weeks** after it has been released (by Thursday February 17th at 23:59). Please notice that finishing the programming exercises can take significant amount of time (especially if you don't have yet much programming experience). Hence, it is recommended that you start immediately working on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Written assignment (10 points)\n",
    "\n",
    "In the \"Economic inequalities, growth and green economy\" and the \"Spatial regression\" lessons + tutorial \n",
    "this week, we went through different indicators that can be used to understand economic inequalities and how \n",
    "(spatial) regression models can be used e.g. to understand which factors influence pricing of Airbnb listings. \n",
    "\n",
    "Write 0.5-2 pages (A4) of text in English where you explain:\n",
    " \n",
    " - What are the key characteristics of different indicators that aim to measure economic inequality in the world?\n",
    " - Why growth/progress indicators that focus purely on economic performance are problematic? \n",
    " - Why is it important to take spatial effects into account when doing regression analysis? \n",
    " - What two approaches did we use to consider spatial effects in the tutorial? How do they differ? \n",
    " \n",
    "Use the lesson materials and the recommended readings (optional) as a source of information for answering to these.\n",
    "\n",
    "----------------\n",
    "\n",
    "## My answer\n",
    "\n",
    "Add your text here.\n",
    "\n",
    "*Hint: To \"activate\" this cell in Editing mode, double click this cell. If you want to get this cell back in the \"Reading-mode\", press Shift+Enter.*\n",
    "\n",
    "\n",
    "## Hints\n",
    "\n",
    "- If you need help in Markdown formatting (e.g. how to add headings, bold, italics, links etc.), please take a look at this excellent [guide / cheatsheet](https://www.markdownguide.org/cheat-sheet/) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Programming assignment (9 points)\n",
    "\n",
    "In this exercise, we practice doing spatial regression using postal code level data from Finland that is openly available via Statistics Finland. We aim to understand what factors influence the housing prices in Helsinki Region. In this exercise, you will learn how to:\n",
    "\n",
    " - investigate linear relationship between different attributes\n",
    " - create spatial weights using pysal library\n",
    " - investigate spatial autocorrelation in the data using Moran's I indicator\n",
    " - conduct Ordinary Least Squares regression and spatial regression models using pysal library \n",
    "\n",
    "\n",
    "### Due date\n",
    "\n",
    "The exercise should be returned by the end of Friday (19th of February, 2021).  \n",
    "\n",
    "### Start your exercise in CSC Notebooks\n",
    "\n",
    "Before you can start programming, you need to launch the CSC Notebook instance and clone your Exercise repository there.\n",
    "If you need help with this, [read the documentation on the course site](https://sustainability-gis.readthedocs.io/en/latest/lessons/L1/git-basics.html).\n",
    " \n",
    "### Hints \n",
    "\n",
    "If there are general questions arising from this exercise, we will add hints to the course website under [Exercise 4 description](https://sustainability-gis.readthedocs.io/en/latest/lessons/L4/exercise-4.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "In this Exercise, we will use openly available postal code level data from Statistics Finland. We use two different datasets: \n",
    "  1. [Paavo postal code level data](https://www.stat.fi/tup/paavo/paavon_aineistokuvaukset_en.html) - Provides us many useful attributes that we can use as explanatory variables in our linear regression models.  \n",
    "  2. [Average prices of dwellings](https://www.stat.fi/til/ashi/index_en.html) in housing companies in Finland (â‚¬ per square meter)\n",
    "  3. OpenStreetMap data - Provides us street network data that we use to calculate an accessibility index for each postal code area which is used as one explanatory variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "In this exercise, we hypothesize that following attributes might explain the average apartment price on a postal code level:\n",
    "\n",
    " 1. Distance to city center\n",
    " 2. Number of jobs in the area\n",
    " 3. Number of people living in the area with higher education degree\n",
    " 4. Average size of the households (i.e. how many people live in an household)\n",
    " 5. Average income of the households\n",
    " \n",
    "In addition to these, we finally consider using a spatial lag model that:\n",
    "\n",
    " 6. The average price of the neighboring areas influences the price on a given area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Prepare data (1 point)\n",
    "\n",
    "In this problem you should:\n",
    "\n",
    "1. Read the postal code level data from Statistics Finland and select the data for Helsinki Region based on municipality code (`kunta`)\n",
    "2. Remove unnecessary columns by selecting only following attributes from the postal code data:\n",
    "\n",
    "  - tp_tyopy --> Number of jobs in the area (total)\n",
    "  - ko_yl_kork --> Number of people having higher education degree\n",
    "  - te_takk --> Average size of the households\n",
    "  - tr_ktu --> Average income of the households\n",
    "\n",
    "\n",
    "3. Read the apartment price data from `apartment_prices_finland_2019.csv` file which is located in the `data` directory (when reading the data, ensure that the data type of the `postal_code` attribute is string. Hint: Check the pandas documentation for read_csv() and the parameter `dtype`. \n",
    "\n",
    "4. Make a table join between the postal code dataset and the apartment price dataset based on the `posti_alue` and `postal_code` attributes. \n",
    "5. Investigate your data. Are there missing values in any of the selected attributes? If there are:\n",
    "\n",
    "  - remove NaN values\n",
    "  - reset the index\n",
    "\n",
    "6. Make map out of the average price per square meter in Helsinki Region. As a result you should have something like following:\n",
    "\n",
    "![Average housing prices](img/housing_prices.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6ce1a1a8258ccce554496a69bcc1b1",
     "grade": true,
     "grade_id": "cell-9ac43772eb2f3d41",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# URL for postal code level data from Statistics Finland\n",
    "url = \"http://geo.stat.fi/geoserver/postialue/wfs?request=GetFeature&typename=postialue:pno_tilasto_2019&outputformat=JSON\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Create an accessibility index for each postal code area (2 points)\n",
    "\n",
    "In this problem, the idea is to create an accessibility index for our postal code areas based on the network distance to the Helsinki city center. You should:\n",
    "\n",
    "1. Calculate network distance from all postal code area centroids to the Helsinki Railway station --> Use pyrosm, NetworkX and OSMnx as we have learned during previous weeks. Sanity check: if you plot the network distances against the price information (as a scatter plot, check [Seaborn lmplot()](https://seaborn.pydata.org/generated/seaborn.lmplot.html)), you should get something like below:\n",
    "\n",
    "![Network distances agains Housing prices](img/price_against_distance.PNG)\n",
    "\n",
    "2. Create an accessibility index as a new column called `access_index` in which you should:\n",
    "  \n",
    "  - Calculate the inverse distance with formula: `abs( distance - max(distance) )`\n",
    "  - Standarize the inversed distance to scale 0-1 with formula: `inverse_distance / max(inverse_distance)`\n",
    "\n",
    "As a result for the accessibility index, you should have something like following:\n",
    "\n",
    "![Accessibility index](img/access_index.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0883ebfaeb3c857c3ef8c3738648f576",
     "grade": true,
     "grade_id": "cell-ecd8d816cb65362d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Make a correlation matrix (1 points)\n",
    "\n",
    "In this problem, we start our statistical analysis by investigating the correlation between the price and the variables we have selected. One of the assumptions of Ordinary Least Squares regression is that there should be linear relationship betweem variables. Hence, before doing anything else, it is good to check whether this assumption You should:\n",
    "\n",
    "1. Calculate a correlation matrix between all the variables in our model (i.e. price, access_index, tp_tyopy, ko_yl_kork, te_takk, tr_ktu) and store the result in a new DataFrame called `correlation_matrix`. Round the values so that they have only 2 decimals. Hint: Check the pandas documentation for `corr()` method.\n",
    "2. Visualize the correlation matrix as a heatmap using Seaborn's [heatmap()](https://seaborn.pydata.org/generated/seaborn.heatmap.html) functionality. You should annotate the correlation values so that they are visible in the plot, and answer to following questions:\n",
    "\n",
    "  - Which three (3) variables have the strongest correlation with price, and what are their correlation coefficients?\n",
    "  - Which variable you might want to drop due to low correlation with price? (Note: do not drop the variable from your data, only answer to the question based on your understanding)\n",
    "  - Multicollinearity should be avoided in OLS, meaning that there shouldn't be a relationship between the explanatory variables. One way to detect multicollinearity is to investigate whether there are high correlation values between the explanatory variables (a typical \"rule of thumb\" cutoff value is 0.8, although lower thresholds are used as well). Based on the correlation matrix, do you see issues with multicollinearity in our variables? Justify your answer with a sentence or two.   \n",
    "  \n",
    "As a result, you should have something like below (*correlation values for price are hidden*):\n",
    "\n",
    "![Correlation matrix ](img/correlation_matrix.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a4fdc84e0325b4fd6ac3c7e41412d76",
     "grade": true,
     "grade_id": "cell-228f81d6b236b4f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - Is there spatial autocorrelation in the price values? (2 points)\n",
    "\n",
    "As we learned during the lesson this week, spatial autocorrelation is something that can influence quite a bit how well our statistical models work. Hence, it is good to try to understand if our dependent variable (*price*) have spatial autocorrelation. Based on the map from Problem 1, we can already see that our values seem to cluster. But as humans are very good at seeing patterns (even if there wouldn't be), it is always good to investigate more analytically (using here `Moran's I` indicator), whether the values have spatial correlation or not. To do this, you should:\n",
    "\n",
    "1. Create spatial weights based on how the boundaries of our postal code areas touch each other. Create the weights based on the data GeoDataFrame using **Queen contiguity** and store the resulting weights into a variable `w`. For creating the spatial weights, you can use the `weights` submodule from `pysal` library (see [docs](https://pysal.org/libpysal/api.html)). If you need further information, we also recommend checking [chapter 4](https://geographicdata.science/book/notebooks/04_spatial_weights.html) from \"Geographic Data Science with Python\" book (Rey et al. *forthcoming*). Also it highly recommended to check the [lesson video 4.3](https://sustainability-gis.readthedocs.io/en/latest/lessons/L4/lesson-overview.html#lesson-videos) starting approximately at 25:30 for additional details about spatial weights.\n",
    "\n",
    "Sanity check (optional): If you plot the spatial weights (i.e. variable `w`) with the postal code areas, you should get something like following as a result (hint: you can plot the weights as you would plot any GeoDataFrame):\n",
    "\n",
    "![Queen contiguity](img/queen_contiguity_weights.PNG)\n",
    "\n",
    "2. Row standardize the weights as was shown during the tutorial this week.\n",
    "\n",
    "3. In our data, there are islands. Which indices of our data represent islands? \n",
    "\n",
    "4. Remove the rows from our GeoDataFrame representing islands (check lesson video) by dropping the rows based on the island indices. Also reset the index at this point.\n",
    "\n",
    "5. Based on this *cleaned data*, recreate the Queen contiguity weights and store them again in variable `w` and row standardize the weights\n",
    "\n",
    "6. Calculate the `Moran's I` based on the \"price\" attribute and using the spatial weights that we created in the previous step. For doing this, you can use the [Moran()](https://pysal.org/esda/generated/esda.Moran.html) function from the pysal library, which accepts the Series of our price column as one parameter and the weights as another, check the pysal docs for details. What is the global Moran's I for our data?\n",
    "\n",
    "7. Create a Moran plot based on our data that allows us to investigate the spatial autocorrelation visually. For doing this, you can use a [plot_moran()](https://splot.readthedocs.io/en/latest/generated/splot.esda.plot_moran.html) -function from pysal's splot submodule. Sanity check: if everything is correct Moran plot should produce something like following (*Moran's I value is hidden*):\n",
    "\n",
    "![Moran plot](img/moran_plot.PNG)\n",
    "\n",
    "\n",
    "**How to read the plot?** The plot on the right displays a positive relationship between both variables (it also shows the global Moran index in the title). This means that we have positive spatial autocorrelation in our data as similar values tend to be located close to each other: high values tend to be close to other high values, and low values tend to close to other low values. On the left plot we can see, how the distribution of our data should look like if the data would be spatially random. The blue vertical line (at x-axis position 0) shows where the mean is in spatially random distribution, and the red vertical line on the very right shows where the Moran's I of our data is. I.e. it is clearly beyond random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4edc4d08b5cad122ac4a03b8a44a7417",
     "grade": true,
     "grade_id": "cell-cf103fa3ea3d9e03",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 - Spatial regression (3 points)\n",
    "\n",
    "\n",
    "As an overview, in this problem, we will:\n",
    "\n",
    "1. investigate the linear relationship between our variables using scatter plots with fitted regression line\n",
    "\n",
    "2. Do log transformations to some of our variables (to ensure the relationship between dependent and explanatory variable is ~ linear)\n",
    "\n",
    "3. conduct three different regression models:\n",
    "\n",
    " - A normal Ordinary Least Squares (OLS) regression model\n",
    " - Exogenous effects SLX model (i.e. spatially lagging explanatory variables)\n",
    " - Spatial lag model (i.e. considering the price of the neighboring areas also as an explanatory variable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Investigate whether our explanatory variables seem to have linear relationship with the \"price\":\n",
    "\n",
    "1. Create a scatter matrix based on our model attributes (price, access_index, tp_tyopy, ko_yl_kork, te_takk, tr_ktu) and fit a regression line to the plot. Use seaborn [pairplot()](https://seaborn.pydata.org/generated/seaborn.pairplot.html#seaborn.pairplot) function to make the visualization. Hint: you can use the parameter `\"kind\"` to specify which kind of plot you want to visualize. For fitting a regression trend line to your plot, you can specify that the kind of plot you want is \"reg\" ([read more here](https://seaborn.pydata.org/tutorial/regression.html#plotting-a-regression-in-other-contexts)). \n",
    "\n",
    "Sanity check: The first row in the result should look something like following:\n",
    "\n",
    "![Scatter matrix with trend line](img/pairplot.PNG)\n",
    "\n",
    "2. Looking at the relationship between the price and other variables, which of the variables seem to have a clear linear relationship with the price? Justify your answer with a sentence or two.\n",
    "\n",
    "3. Based on the scatter plot, which of the variables seem to have a negative relationship against the \"price\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "216c1760582675a5ca0a571e8b51ed10",
     "grade": true,
     "grade_id": "cell-b76a070dd866d3e5",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Logarithmic transformation of variables in a regression model is a common way to handle situations where a non-linear relationship exists between the independent and dependent variables. Based on the insights from the previous step, we might want to do a log transformation to some of our attributes in order to make the relationship between attributes linear. Using a log transformation also helps with heavy skewness in our data (e.g. looking at the price histogram shows that the values are heavily skewed to the left side of the histogram).\n",
    "\n",
    "In this step, you should:\n",
    "\n",
    "1. Make a logarithmic transformation (as shown in the [tutorial](https://sustainability-gis.readthedocs.io/en/latest/lessons/L4/spatial_regression.html#baseline-nonspatial-regression)) to following attributes and store the values as shown below (making the attribute names easier to understand):\n",
    "\n",
    " - \"price\" --> \"log_price\" (the column name to which you should store the result)\n",
    " - \"tp_tyopy\" --> \"log_n_jobs\"\n",
    " - \"ko_yl_kork\" --> \"log_high_edu\"\n",
    " - \"te_takk\" --> \"log_avg_household_size\"\n",
    " - \"tr_ktu\" --> \"log_avg_income\"\n",
    " \n",
    " \n",
    "2. Create another scatter matrix in a similar manner as in step 1, but now use the log transformed attributes instead of the original ones (notice that we didn't log transform the accessibility index which you should also include in the plot). \n",
    "\n",
    "3. Looking at the scatter matrix, what happened to the histograms of your variables? Explain with a sentence or two.\n",
    "4. Looking at the scatter matrix, does the log transformed explanatory variables seem to have clearer linear relationship with the dependent variable? Explain with a sentence or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a08039531c70b4373a8f49685dd764a5",
     "grade": true,
     "grade_id": "cell-8ac3e9487b283437",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 - Ordinary Least Squares (OLS)\n",
    "\n",
    "Here we do our first regression model using Ordinary Least Squares regression:\n",
    "\n",
    "1. Create OLS where the \"log_price\" is the dependent variable and access_index and the other logged variables (log_n_jobs, log_high_edu, log_avg_household_size, log_avg_income) as the explanatory variables\n",
    "2. Print out the summary of the regression model and answer the following questions:\n",
    "  - What is the R-squared value of our model?\n",
    "  - Which of the explanatory variables has the highest coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "757411c23f6d80b6092370b2100dcc03",
     "grade": true,
     "grade_id": "cell-8c6e815752d31aea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Exogenous effects, SLX model \n",
    "\n",
    "In this step, we will learn how introducing spatial effects into the explanatory variables influences our regression model. You should:\n",
    "\n",
    "1. Create row standardized Queen contiguity spatial weights based on our GeoDataFrame (alternatively, you can use the same weights which were done in problem 4) \n",
    "2. Create spatially lagged versions of our explanatory variables using the `weights.spatial_lag.lag_spatial()` function of pysal, i.e. the same approach which was introduced in the [tutorial](https://sustainability-gis.readthedocs.io/en/latest/lessons/L4/spatial_regression.html#spatially-lagged-exogenous-regressors-wx) and store them as columns into the data, named as follows:\n",
    "\n",
    " - \"access_index\" --> \"w_access_index\"\n",
    " - \"log_n_jobs\" --> \"w_log_n_jobs\"\n",
    " - \"log_high_edu\" --> \"w_log_high_edu\"\n",
    " - \"log_avg_household_size\" --> \"w_log_avg_household_size\"\n",
    " - \"log_avg_income\" --> \"w_log_avg_income\"\n",
    " \n",
    "3. Do a regular OLS regression in a similar manner as in Step 2 (having \"log_price\" as the dependent variable), but now use these spatially lagged attributes as explanatory variables. Print the summary of the model and answer to following questions:\n",
    " - What is the R-squared value of our model? Did it improve?\n",
    " - Which of the explanatory variables has the highest coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2042638287381aa0555aab102f97e502",
     "grade": true,
     "grade_id": "cell-7f55491cef622042",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Spatial lag model\n",
    "\n",
    "\n",
    "In this step, we will learn how introducing spatial effects also into the dependent variables influence our regression model (i.e. we will do a spatial lag model). You should:\n",
    "\n",
    "1. Create row standardized Queen contiguity spatial weights based on our GeoDataFrame (alternatively, you can use the same weights which were used in previous step) \n",
    "2. Conduct a spatial two stage least squares regression (i.e. a spatial lag model) in a similar manner as shown [in the tutorial](https://sustainability-gis.readthedocs.io/en/latest/lessons/L4/spatial_regression.html#spatially-lagged-endogenous-regressors-wy). This time, you should use the same explanatory variables as in our first OLS model, i.e. access_index, log_n_jobs, log_high_edu, log_avg_household_size, and log_avg_income. Print the summary of the model and answer to following questions:\n",
    " - What is the Pseudo R-squared value of our model? \n",
    " - Which of the explanatory variables has the highest coefficient? Does it make sense to you? Explain with a sentence or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30bbe42bbba9a596c934729a57b5080e",
     "grade": true,
     "grade_id": "cell-270782785fada9c0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 - How long did it take? Optional feedback (1 point)\n",
    "\n",
    "To help developing the exercises, and understanding the time that it took for you to finish the Exercise, please provide an estimate of how many hours you spent for doing this exercise? *__Hint:__ To \"activate\" this cell in Editing mode, double click this cell. If you want to get this cell back in the \"Reading-mode\", press Shift+Enter.*\n",
    "\n",
    "I spent approximately this many hours: **X hours**\n",
    "\n",
    "In addition, if you would like to give any feedback about the exercise (optional), please provide it below:\n",
    "\n",
    "**My feedback:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
